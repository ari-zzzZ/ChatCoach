{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099699e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"./DPO_2\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f53ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftMixedModel(\n",
       "  (base_model): MixedModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                  (stage2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  (stage2): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  (stage2): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                  (stage2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  (stage2): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  (stage2): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                  (stage2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  (stage2): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  (stage2): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                  (stage2): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  (stage2): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  (stage2): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftMixedModel\n",
    "# 加载基座\n",
    "base = AutoModelForCausalLM.from_pretrained(\"D:/CoachBot/SFTuned\", torch_dtype=\"auto\", device_map=\"cuda\")\n",
    "\n",
    "# 第一次载入 adapter → 名叫 \"default\"\n",
    "model = PeftMixedModel.from_pretrained(base, \"./DPO_1\")\n",
    "\n",
    "# 再载入第二个 adapter → 名叫 \"stage2\"\n",
    "model.load_adapter(\"./DPO_2\", adapter_name=\"stage2\")\n",
    "\n",
    "# 激活\n",
    "model.set_adapter([\"default\", \"stage2\"])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import torch\n",
    "# 2. 单轮对话函数\n",
    "def chat_qwen3(messages, \n",
    "               enable_thinking: bool = True, \n",
    "               max_new_tokens: int = 512):\n",
    "    \"\"\"\n",
    "    messages: List[Dict[str, str]]，格式如 [{\"role\":\"user\",\"content\": \"...\"}]\n",
    "    enable_thinking: 是否开启 <think> 模式\n",
    "    \"\"\"\n",
    "    # 2.1 应用聊天模板，拼接 prompt + 回复提示\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking\n",
    "    )\n",
    "    # 2.2 编码并送入模型\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    # 2.3 生成\n",
    "    generation = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.8,\n",
    "        top_k=20\n",
    "    )\n",
    "    # 2.4 切掉输入部分，只保留模型新生成的 token\n",
    "    output_ids = generation[0][len(inputs.input_ids[0]):]\n",
    "    return tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "# 3. 测试单轮\n",
    "messages = [{\"role\": \"user\", \"content\": \"我好难受。\"}]\n",
    "response = chat_qwen3(messages)\n",
    "print(\"Bot:\", response)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018545da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch\n",
    "# 4. 多轮示例（可选）\n",
    "class QwenChatbot:\n",
    "    def __init__(self, \n",
    "                 model=model,\n",
    "                 system_prompt=\"你是一位心理咨询师。\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model,\n",
    "                                                          torch_dtype=torch.float16,\n",
    "                                                          device_map=\"auto\")\n",
    "        self.history = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    def generate(self, user_input, enable_thinking=True):\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            self.history,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "        gen = self.model.generate(**inputs, max_new_tokens=512)\n",
    "        out_ids = gen[0][len(inputs.input_ids[0]):]\n",
    "        reply = self.tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply\n",
    "\n",
    "        # 使用多轮 Chatbot\n",
    "bot = QwenChatbot()\n",
    "print(\"Bot:\", bot.generate(\"我好难受\", enable_thinking=True))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bot = QwenChatbot(model)\n",
    "    print(\"=== Qwen3 Chat ===\")\n",
    "    while True:\n",
    "        q = input(\"You: \")\n",
    "        if q.strip().lower() in (\"exit\", \"quit\"):\n",
    "            print(\"Bye\")\n",
    "            break\n",
    "        # 你可以根据需要切换 thinking 模式\n",
    "        ans = bot.generate(q, enable_thinking=False)\n",
    "        print(\"Bot:\", ans)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb219238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Qwen3 Chat ===\n",
      "Bot: 你好，我听到你说“好难受”。这说明你可能正在经历一些痛苦或不适。我想，我们可以先来个简单的对话，了解更多的背景信息和感受，然后根据这些信息来设计一个合适的心理咨询方案。咨询过程是安全的，你可以随时退出，或者向其他咨询者求助。最重要的是，我希望你能在这个过程中找到一个能够理解和支持你的的人。谢谢你的信任，也希望你能从中学到一些东西。下一次，我们准备好了吗？\n",
      "Bot: 你好，很高兴见到你。心理咨询是一种专业领域，旨在帮助个体或群体解决内心的问题、提升自我意识、促进成长与发展。在这里，你可以得到一个安全、保密、专业的服务。我们还需要一个合格的咨询师，这由你的选择决定。因此，在这里，我们可以暂时放下之前的对话，专注于当前的互动。下次，当你再次提到“好难受”时，我将再次向你承诺，确保服务是安全、有效的。记住，心理咨询并不是解决问题的万能药，但它能为许多其他方面带来积极的变化。谢谢你，期待着与你的对话。下一次见，祝你一切顺利。\n",
      "Bot: 谢谢你分享了你的感受，这让我能够更好地了解和回应你的需求。之前的对话只是用来建立一个安全的咨询环境，之后的对话将由你主导，由你提出问题和要求，由我协助执行。因此，现在可以随时结束对话，或者向其他咨询者求助。记住，心理咨询是一个持续的过程，需要时间和耐心。期待着与你的再次相遇，祝你一切顺利。下次见，你好。\n",
      "Bye\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
